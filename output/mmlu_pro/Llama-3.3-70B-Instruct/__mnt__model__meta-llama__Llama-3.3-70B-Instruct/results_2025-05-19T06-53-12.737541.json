{
  "results": {
    "mmlu_pro": {
      "exact_match,custom-extract": 0.7117686170212766,
      "exact_match_stderr,custom-extract": 0.004053764575856132,
      "alias": "mmlu_pro"
    },
    "mmlu_pro_biology": {
      "alias": " - biology",
      "exact_match,custom-extract": 0.8605299860529986,
      "exact_match_stderr,custom-extract": 0.012946933436697634
    },
    "mmlu_pro_business": {
      "alias": " - business",
      "exact_match,custom-extract": 0.7452471482889734,
      "exact_match_stderr,custom-extract": 0.015521960710052789
    },
    "mmlu_pro_chemistry": {
      "alias": " - chemistry",
      "exact_match,custom-extract": 0.7243816254416962,
      "exact_match_stderr,custom-extract": 0.013286374629983121
    },
    "mmlu_pro_computer_science": {
      "alias": " - computer_science",
      "exact_match,custom-extract": 0.7414634146341463,
      "exact_match_stderr,custom-extract": 0.02164931770175752
    },
    "mmlu_pro_economics": {
      "alias": " - economics",
      "exact_match,custom-extract": 0.792654028436019,
      "exact_match_stderr,custom-extract": 0.013962907124731161
    },
    "mmlu_pro_engineering": {
      "alias": " - engineering",
      "exact_match,custom-extract": 0.5634674922600619,
      "exact_match_stderr,custom-extract": 0.015940614139966947
    },
    "mmlu_pro_health": {
      "alias": " - health",
      "exact_match,custom-extract": 0.7347188264058679,
      "exact_match_stderr,custom-extract": 0.015445528422903948
    },
    "mmlu_pro_history": {
      "alias": " - history",
      "exact_match,custom-extract": 0.6876640419947506,
      "exact_match_stderr,custom-extract": 0.02377427885575152
    },
    "mmlu_pro_law": {
      "alias": " - law",
      "exact_match,custom-extract": 0.5304268846503178,
      "exact_match_stderr,custom-extract": 0.015047627559624212
    },
    "mmlu_pro_math": {
      "alias": " - math",
      "exact_match,custom-extract": 0.772020725388601,
      "exact_match_stderr,custom-extract": 0.011418137778991843
    },
    "mmlu_pro_other": {
      "alias": " - other",
      "exact_match,custom-extract": 0.7132034632034632,
      "exact_match_stderr,custom-extract": 0.014886507567599004
    },
    "mmlu_pro_philosophy": {
      "alias": " - philosophy",
      "exact_match,custom-extract": 0.6392785571142284,
      "exact_match_stderr,custom-extract": 0.0215187171506013
    },
    "mmlu_pro_physics": {
      "alias": " - physics",
      "exact_match,custom-extract": 0.7159353348729792,
      "exact_match_stderr,custom-extract": 0.012517227916962687
    },
    "mmlu_pro_psychology": {
      "alias": " - psychology",
      "exact_match,custom-extract": 0.7794486215538847,
      "exact_match_stderr,custom-extract": 0.014686539382575017
    }
  },
  "groups": {
    "mmlu_pro": {
      "exact_match,custom-extract": 0.7117686170212766,
      "exact_match_stderr,custom-extract": 0.004053764575856132,
      "alias": "mmlu_pro"
    }
  },
  "group_subtasks": {
    "mmlu_pro": [
      "mmlu_pro_biology",
      "mmlu_pro_business",
      "mmlu_pro_chemistry",
      "mmlu_pro_computer_science",
      "mmlu_pro_economics",
      "mmlu_pro_engineering",
      "mmlu_pro_health",
      "mmlu_pro_history",
      "mmlu_pro_law",
      "mmlu_pro_math",
      "mmlu_pro_other",
      "mmlu_pro_philosophy",
      "mmlu_pro_physics",
      "mmlu_pro_psychology"
    ]
  },
  "configs": {
    "mmlu_pro_biology": {
      "task": "mmlu_pro_biology",
      "task_alias": "biology",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088aa5580>, subject='biology')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088aa4fe0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about biology. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088aa4c20>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_business": {
      "task": "mmlu_pro_business",
      "task_alias": "business",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088a37c40>, subject='business')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a345e0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about business. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a36700>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_chemistry": {
      "task": "mmlu_pro_chemistry",
      "task_alias": "chemistry",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088a377e0>, subject='chemistry')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a37100>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about chemistry. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a367a0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_computer_science": {
      "task": "mmlu_pro_computer_science",
      "task_alias": "computer_science",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088a34360>, subject='computer science')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a34040>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about computer science. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a35080>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_economics": {
      "task": "mmlu_pro_economics",
      "task_alias": "economics",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088a35e40>, subject='economics')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a35260>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about economics. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a35a80>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_engineering": {
      "task": "mmlu_pro_engineering",
      "task_alias": "engineering",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088a34d60>, subject='engineering')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a34ae0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about engineering. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088a347c0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_health": {
      "task": "mmlu_pro_health",
      "task_alias": "health",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088c3f6a0>, subject='health')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3fa60>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about health. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3f880>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_history": {
      "task": "mmlu_pro_history",
      "task_alias": "history",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088c3d8a0>, subject='history')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3c9a0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about history. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3cae0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_law": {
      "task": "mmlu_pro_law",
      "task_alias": "law",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088c3e340>, subject='law')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3e520>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about law. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3dda0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_math": {
      "task": "mmlu_pro_math",
      "task_alias": "math",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088c3d080>, subject='math')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3d260>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about math. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3cc20>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_other": {
      "task": "mmlu_pro_other",
      "task_alias": "other",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088bce5c0>, subject='other')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088bce340>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about other. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088c3cf40>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_philosophy": {
      "task": "mmlu_pro_philosophy",
      "task_alias": "philosophy",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088bcf560>, subject='philosophy')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088bcf420>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about philosophy. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088bcede0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_physics": {
      "task": "mmlu_pro_physics",
      "task_alias": "physics",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74c088bcee80>, subject='physics')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088bceac0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about physics. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74c088bce980>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    },
    "mmlu_pro_psychology": {
      "task": "mmlu_pro_psychology",
      "task_alias": "psychology",
      "dataset_path": "TIGER-Lab/MMLU-Pro",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x74beb4532980>, subject='psychology')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x74beb4532b60>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about psychology. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x74beb4532e80>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 1,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<|eot|>"
        ],
        "max_gen_toks": 32768,
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(?i)(?:FINAL ANSWER: |The best answer is |Answer\\s*:|Answer\\s*:​​​​​​|উত্তর\\s*:|उत्तर\\s*:|উত্তরঃ|উত্তর\\s*:|Antwort\\s*:|답변\\s*:|정답\\s*:|답\\s*:|答案\\s*：|答案\\s*:|答\\s*：|答\\s*:|答复\\s*：|答曰\\s*：|الإجابة:|الجواب:|إجابة:|الإجابة النهائية:|الإجابة الصحيحة:|الإجابة الصحيحة هي:|الإجابة هي:|Respuesta\\s*:|Risposta\\s*:|答え\\s*:|答え\\s*：|回答\\s*:|回答\\s*：|解答\\s*:|Jawaban\\s*:|Réponse\\s*:|Resposta\\s*:|Jibu\\s*:|Idahun\\s*:|Ìdáhùn\\s*:|Idáhùn\\s*:|Àmọ̀nà\\s*:|Àdáhùn\\s*:|Ànúgọ\\s*:|Àṣàyàn\\s*:)\\s*\\(?([A-J]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 2.1,
        "model": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "pretrained": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
        "tensor_parallel_size": 8,
        "max_model_len": 32768,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": true
      }
    }
  },
  "versions": {
    "mmlu_pro": 2.0,
    "mmlu_pro_biology": 2.1,
    "mmlu_pro_business": 2.1,
    "mmlu_pro_chemistry": 2.1,
    "mmlu_pro_computer_science": 2.1,
    "mmlu_pro_economics": 2.1,
    "mmlu_pro_engineering": 2.1,
    "mmlu_pro_health": 2.1,
    "mmlu_pro_history": 2.1,
    "mmlu_pro_law": 2.1,
    "mmlu_pro_math": 2.1,
    "mmlu_pro_other": 2.1,
    "mmlu_pro_philosophy": 2.1,
    "mmlu_pro_physics": 2.1,
    "mmlu_pro_psychology": 2.1
  },
  "n-shot": {
    "mmlu_pro_biology": 1,
    "mmlu_pro_business": 1,
    "mmlu_pro_chemistry": 1,
    "mmlu_pro_computer_science": 1,
    "mmlu_pro_economics": 1,
    "mmlu_pro_engineering": 1,
    "mmlu_pro_health": 1,
    "mmlu_pro_history": 1,
    "mmlu_pro_law": 1,
    "mmlu_pro_math": 1,
    "mmlu_pro_other": 1,
    "mmlu_pro_philosophy": 1,
    "mmlu_pro_physics": 1,
    "mmlu_pro_psychology": 1
  },
  "higher_is_better": {
    "mmlu_pro": {
      "exact_match": true
    },
    "mmlu_pro_biology": {
      "exact_match": true
    },
    "mmlu_pro_business": {
      "exact_match": true
    },
    "mmlu_pro_chemistry": {
      "exact_match": true
    },
    "mmlu_pro_computer_science": {
      "exact_match": true
    },
    "mmlu_pro_economics": {
      "exact_match": true
    },
    "mmlu_pro_engineering": {
      "exact_match": true
    },
    "mmlu_pro_health": {
      "exact_match": true
    },
    "mmlu_pro_history": {
      "exact_match": true
    },
    "mmlu_pro_law": {
      "exact_match": true
    },
    "mmlu_pro_math": {
      "exact_match": true
    },
    "mmlu_pro_other": {
      "exact_match": true
    },
    "mmlu_pro_philosophy": {
      "exact_match": true
    },
    "mmlu_pro_physics": {
      "exact_match": true
    },
    "mmlu_pro_psychology": {
      "exact_match": true
    }
  },
  "n-samples": {
    "mmlu_pro_biology": {
      "original": 717,
      "effective": 717
    },
    "mmlu_pro_business": {
      "original": 789,
      "effective": 789
    },
    "mmlu_pro_chemistry": {
      "original": 1132,
      "effective": 1132
    },
    "mmlu_pro_computer_science": {
      "original": 410,
      "effective": 410
    },
    "mmlu_pro_economics": {
      "original": 844,
      "effective": 844
    },
    "mmlu_pro_engineering": {
      "original": 969,
      "effective": 969
    },
    "mmlu_pro_health": {
      "original": 818,
      "effective": 818
    },
    "mmlu_pro_history": {
      "original": 381,
      "effective": 381
    },
    "mmlu_pro_law": {
      "original": 1101,
      "effective": 1101
    },
    "mmlu_pro_math": {
      "original": 1351,
      "effective": 1351
    },
    "mmlu_pro_other": {
      "original": 924,
      "effective": 924
    },
    "mmlu_pro_philosophy": {
      "original": 499,
      "effective": 499
    },
    "mmlu_pro_physics": {
      "original": 1299,
      "effective": 1299
    },
    "mmlu_pro_psychology": {
      "original": 798,
      "effective": 798
    }
  },
  "config": {
    "model": "vllm",
    "model_args": "model=/mnt/model/meta-llama/Llama-3.3-70B-Instruct,pretrained=/mnt/model/meta-llama/Llama-3.3-70B-Instruct,tensor_parallel_size=8,max_model_len=32768,gpu_memory_utilization=0.9,trust_remote_code=true",
    "batch_size": "auto",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": {
      "max_gen_toks": 32768,
      "until": [
        "<|eot|>"
      ],
      "temperature": 0.0
    },
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "98463aa375b1799449708448c4cf8bc4f5593dbc",
  "date": 1747661311.436898,
  "pretty_env_info": "PyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 570.124.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               208\nOn-line CPU(s) list:                  0-207\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480+\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   52\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\nVirtualization:                       VT-x\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            6.5 MiB (208 instances)\nL1i cache:                            6.5 MiB (208 instances)\nL2 cache:                             416 MiB (104 instances)\nL3 cache:                             32 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-103\nNUMA node1 CPU(s):                    104-207\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] triton==3.2.0\n[conda] Could not collect",
  "transformers_version": "4.51.3",
  "lm_eval_version": "0.4.8",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|finetune_right_pad_id|>",
    "128004"
  ],
  "tokenizer_eos_token": [
    "<|eot_id|>",
    "128009"
  ],
  "tokenizer_bos_token": [
    "<|begin_of_text|>",
    "128000"
  ],
  "eot_token_id": 128009,
  "max_length": 32768,
  "task_hashes": {
    "mmlu_pro_biology": "5e9a8bc778c0661dfe1e9ae03a7eab792f1224d74e8455928f5fa0c9963bcb90",
    "mmlu_pro_business": "05e41edeac27565f4c9e9b71d564153fa95beb8fc29fc0ea09bd2fdffaec2136",
    "mmlu_pro_chemistry": "7b46ba488fceb8c86deb41451b1069d6012a22ab3df2c1af08c61fbc9d9cbbc2",
    "mmlu_pro_computer_science": "29a59ab148fd50667954f30ac74901d30423a87449dc2575240ff9351b340c71",
    "mmlu_pro_economics": "7b8ac742848398da7a3349aa5f01edffb5921f662a1865887c99f4feb048df03",
    "mmlu_pro_engineering": "87db8dfa331805647f496b6e8a6a2de88a5b900ed76882dc06441524de26b22d",
    "mmlu_pro_health": "8f54d3d8ac6ee5bd9684f2859b5af220c7b004d09ab64569c7a1abb7fd6072f5",
    "mmlu_pro_history": "9a2b06f7daeedf97eb19d3ffa65111ea89f64ea4d6bdd552b62f9c8cb51eb725",
    "mmlu_pro_law": "f8bebfcda16ac5e33f6aaf1f6df9d49007782ce25a20437b4ec28909025c2992",
    "mmlu_pro_math": "2c9c2f29cff610a44da861583a54ee17224dedae4b49e4979b4ad0e1b83930d7",
    "mmlu_pro_other": "aa4a1f3675fa1f0f14ee97cfaa3b32fa326c6e483edbefb1af4564995f9e0afe",
    "mmlu_pro_philosophy": "4b9b857671bc0980428e1f64f801877ae9462aa00da55c2b838a0be383221579",
    "mmlu_pro_physics": "c2dffcdd34852db2a7bfe027c6f83dda1b7f14be9cb44bdc82fed3235f4992ea",
    "mmlu_pro_psychology": "f19b1a84ab8c325755624a651b392a76cbbb0d55d271a6551c928b78b6370f2f"
  },
  "model_source": "vllm",
  "model_name": "/mnt/model/meta-llama/Llama-3.3-70B-Instruct",
  "model_name_sanitized": "__mnt__model__meta-llama__Llama-3.3-70B-Instruct",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
  "chat_template_sha": "e10ca381b1ccc5cf9db52e371f3b6651576caee0a630b452e2816b2d404d4b65",
  "start_time": 514775.322428953,
  "end_time": 516273.571668245,
  "total_evaluation_time_seconds": "1498.2492392919958"
}